# @title lab-4.2
import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist
from tensorflow.keras import regularizers

# Load MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()

# Normalize the dataset
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# Flatten the images
x_train = x_train.reshape((x_train.shape[0], -1))
x_test = x_test.reshape((x_test.shape[0], -1))

# Define the size of the encoding (latent space)
encoding_dim = 128  # 128-dimensional encoding

# 1st Autoencoder (Encoder + Decoder)
input_img = Input(shape=(x_train.shape[1],))

# Encoder Layer 1
encoded = Dense(512, activation='relu')(input_img)

# Encoder Layer 2
encoded = Dense(encoding_dim, activation='relu')(encoded)

# Decoder Layer 1
decoded = Dense(512, activation='relu')(encoded)

# Decoder Layer 2
decoded = Dense(x_train.shape[1], activation='sigmoid')(decoded)

# Create the autoencoder model
autoencoder = Model(input_img, decoded)

# Compile the model
autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')

# Train the model
autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))

# Get the encoder part of the model
encoder = Model(input_img, encoded)

# Encode the input images
encoded_imgs = encoder.predict(x_test)

# Now, we can use the encoded_imgs for further tasks like classification, clustering, etc.
print("Encoded images shape:", encoded_imgs.shape)

# You can use the stacked autoencoder in a more advanced pipeline, like using the encoded representations as features for classification models.
