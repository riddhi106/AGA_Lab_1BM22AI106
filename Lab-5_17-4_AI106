# @title 17-4-25_lab-5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neural_network import BernoulliRBM
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 
                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 
                'hours_per_week', 'native_country', 'income']
data = pd.read_csv(url, names=column_names, na_values=" ?", sep=',\s+', engine='python')
data.dropna(inplace=True)
categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le
X = data.drop('income', axis=1).values
y = data['income'].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
n_hidden_1 = 128  # Number of hidden units for the first RBM layer
n_hidden_2 = 64   # Number of hidden units for the second RBM layer
rbm1 = BernoulliRBM(n_components=n_hidden_1, learning_rate=0.1, n_iter=10, random_state=42)
X_train_rbm1 = rbm1.fit_transform(X_train)
rbm2 = BernoulliRBM(n_components=n_hidden_2, learning_rate=0.1, n_iter=10, random_state=42)
X_train_rbm2 = rbm2.fit_transform(X_train_rbm1)
# We will use a simple fully connected neural network (DBN) with two hidden layers.
model_dbn = Sequential()
model_dbn.add(Dense(64, input_dim=n_hidden_2, activation='relu'))  # First hidden layer
model_dbn.add(Dense(32, activation='relu'))  # Second hidden layer
model_dbn.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)
model_dbn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dbn.fit(X_train_rbm2, y_train, epochs=10, batch_size=128)
X_test_rbm1 = rbm1.transform(X_test)
X_test_rbm2 = rbm2.transform(X_test_rbm1)
test_loss, test_acc = model_dbn.evaluate(X_test_rbm2, y_test)
print(f"Test accuracy for DBN: {test_acc * 100:.2f}%")
mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=10, solver='adam', random_state=42)
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)
traditional_acc = accuracy_score(y_test, y_pred)
print(f"Test accuracy for traditional DNN: {traditional_acc * 100:.2f}%")
plt.figure(figsize=(10, 5))
for i in range(10):  # First 10 components
    plt.subplot(2, 5, i + 1)
    plt.imshow(rbm1.components_[i].reshape(2, 7), cmap='gray')  # Changed from (8,8) to (2,7)
    plt.axis('off')
plt.suptitle("RBM1 Learned Features (reshaped 2x7)")
plt.tight_layout()
plt.show()
